---
title: "CRC_ML_NHANES"
author: "Minyang Xu"
date: "`r Sys.Date()`"
output: github_document
---

Import packages
```{r}
library(haven)
library(dplyr)
library(purrr)
library(ggplot2)
library(reshape2)
library(corrplot)
library(broom)
library(caret)
library(pROC)
library(tibble)
```



Set Working Directory & Read Data
```{r}
setwd("/Users/minyangmaxims_xu/Library/CloudStorage/GoogleDrive-mx2269@nyu.edu/My Drive/2024 Fall/GU2686/Repository/Dataset")

# è¯»å–æ•°æ®ï¼ˆä¸å†åˆ é™¤ä»»ä½•å˜é‡ï¼‰
demorgraphic <- read_xpt("Demorgraphic.XPT")
totalcholoesterol <- read_xpt("Cholesterol - Total (P_TCHOL).XPT")
chromiunurine <- read_xpt("Chromium Urine.XPT")
completebloodcount <- read_xpt("Complete Blood Count with 5-Part Differential in Whole Blood (P_CBC).XPT")
medicalcondition <- read_xpt("Medical Conditions (P_MCQ).XPT")
smoking <- read_xpt("Smoking - Cigarette Use (P_SMQ).XPT")
bodymeasure <- read_xpt("Body Measure (P_BMX).XPT")
```

Merge Data by SEQN
```{r}
datasets <- list(
  demorgraphic = demorgraphic,
  totalcholoesterol = totalcholoesterol,
  chromiunurine = chromiunurine,
  completebloodcount = completebloodcount,
  medicalcondition = medicalcondition,
  smoking = smoking,
  bodymeasure = bodymeasure
)
merged_data <- reduce(datasets, ~ inner_join(.x, .y, by = "SEQN"))
head(merged_data)

# æ¸…é™¤ä¸å¿…è¦çš„å˜é‡ï¼Œä»…ä¿ç•™åˆå¹¶åçš„æ•°æ®
rm(list = setdiff(ls(), "merged_data"))
```


Select & Process Features
```{r}
# ğŸ“Œ Select Variables for Analysis
selected_data <- merged_data %>%
  select(SEQN, RIDAGEYR, RIAGENDR, RIDRETH1, LBXTC, LBXWBCSI, LBXLYPCT, 
         LBXEOPCT, LBXRBCSI, LBXHGB, LBXPLTSI, MCQ080, MCQ220, SMQ020, 
         BMXBMI, MCQ230A, MCQ230B, MCQ230C)

# ğŸ“Œ Create Outcome Variable (Disease Status)
selected_data <- selected_data %>%
  mutate(outcome = if_else(MCQ230A == 16 | MCQ230B == 16 | MCQ230C == 16, 1, 0))

# ğŸ“Œ Handle Missing Values in Outcome
selected_data$outcome[is.na(selected_data$outcome)] <- 0

# ğŸ“Œ Remove Redundant Variables (Used in Outcome)
selected_data <- selected_data %>% select(-MCQ230A, -MCQ230B, -MCQ230C)

```


Data Balancing (Downsampling)
```{r}
# ğŸ“Œ Split Data by Outcome
outcome_0 <- selected_data %>% filter(outcome == 0)
outcome_1 <- selected_data %>% filter(outcome == 1)

# ğŸ“Œ Downsample `outcome=0` to 1200 Samples
set.seed(42)
outcome_0_downsampled <- outcome_0 %>% slice_sample(n = 1200)

# ğŸ“Œ Combine Downsampled Data with `outcome=1`
balanced_data <- bind_rows(outcome_0_downsampled, outcome_1) %>% sample_frac(1)

# ğŸ“Œ Check Outcome Distribution
table(balanced_data$outcome)

```



Data Preprocessing
```{r}
# ğŸ“Œ Remove `SEQN` Variable
balanced_data <- balanced_data %>% select(-SEQN)

# ğŸ“Œ Impute Missing Values (Numerical: Mean, Categorical: Mode)
balanced_data <- balanced_data %>%
  mutate(across(where(is.numeric), ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)))

impute_mode <- function(x) {
  mode_val <- names(sort(table(x), decreasing = TRUE))[1]
  x[is.na(x)] <- mode_val
  return(x)
}
balanced_data <- balanced_data %>%
  mutate(across(where(is.factor), impute_mode))

```




Train-Test Split (80/20)
```{r}
# ğŸ“Œ Create Training & Testing Data (80/20 Split)
set.seed(123)
train_index <- createDataPartition(balanced_data$outcome, p = 0.8, list = FALSE)
train_data <- balanced_data[train_index, ]
test_data <- balanced_data[-train_index, ]

```



Logistic Regression Model
```{r}
# ğŸ“Œ Train Logistic Regression Model
logit_model <- glm(outcome ~ ., data = train_data, family = binomial)

# ğŸ“Œ Model Summary
summary(logit_model)

# ğŸ“Œ Predict on Test Data
logit_prob <- predict(logit_model, test_data, type = "response")

# ğŸ“Œ Convert Probability to Binary Predictions
logit_pred <- ifelse(logit_prob > 0.5, 1, 0)
logit_pred <- factor(logit_pred, levels = c(0, 1))
test_data$outcome <- factor(test_data$outcome, levels = c(0, 1))

# ğŸ“Œ Compute Confusion Matrix
conf_matrix <- confusionMatrix(logit_pred, test_data$outcome, positive = "1")
print(conf_matrix)

# ğŸ“Œ Performance Metrics
accuracy <- conf_matrix$overall["Accuracy"]
precision <- conf_matrix$byClass["Precision"]
recall <- conf_matrix$byClass["Sensitivity"]
specificity <- conf_matrix$byClass["Specificity"]
f1_score <- conf_matrix$byClass["F1"]

cat("\nğŸ“Œ **Logistic Regression Performance Metrics**:\n")
cat("âœ… Accuracy:", round(accuracy, 4), "\n")
cat("âœ… Precision:", round(precision, 4), "\n")
cat("âœ… Recall:", round(recall, 4), "\n")
cat("âœ… Specificity:", round(specificity, 4), "\n")
cat("âœ… F1 Score:", round(f1_score, 4), "\n")

# ğŸ“Œ Compute AUC
roc_curve <- roc(test_data$outcome, logit_prob)
auc_value <- auc(roc_curve)
cat("âœ… AUC:", round(auc_value, 4), "\n")

# ğŸ“Œ Plot ROC Curve
ggplot() +
  geom_line(aes(x = roc_curve$specificities, y = roc_curve$sensitivities), color = "blue") +
  geom_abline(linetype = "dashed") +
  labs(title = "ROC Curve - Logistic Regression", x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()




# ğŸ“Œ Extract Feature Importance (Absolute Coefficients)
feature_importance <- broom::tidy(logit_model) %>%
  filter(term != "(Intercept)") %>%
  mutate(abs_coef = abs(estimate)) %>%
  arrange(desc(abs_coef))

# ğŸ“Œ Display Top Features
print(feature_importance)

# ğŸ“Œ Plot Feature Importance
ggplot(feature_importance, aes(x = reorder(term, abs_coef), y = abs_coef)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Feature Importance - Logistic Regression",
       x = "Feature",
       y = "Absolute Coefficient Value") +
  theme_minimal()

```





Random Forest
```{r}
library(caret)       # æ•°æ®é¢„å¤„ç† & è¯„ä¼°
library(randomForest) # Random Forest
library(pROC)        # ROC æ›²çº¿ & AUC
library(ggplot2)     # å¯è§†åŒ–

# â¶ **è½¬æ¢ `outcome` ä¸ºå› å­**
balanced_data$outcome <- as.factor(balanced_data$outcome)

# â· **æ‹†åˆ† `Train/Test` æ•°æ®é›† (80/20)**
set.seed(123)
train_index <- createDataPartition(balanced_data$outcome, p = 0.8, list = FALSE)
train_data <- balanced_data[train_index, ]
test_data <- balanced_data[-train_index, ]

# â¸ **è®­ç»ƒ `Logistic Regression`**
logit_model <- glm(outcome ~ ., data = train_data, family = binomial)
summary(logit_model)

# â¹ **é¢„æµ‹ `Test` æ•°æ®**
logit_prob <- predict(logit_model, test_data, type = "response")
logit_pred <- factor(ifelse(logit_prob > 0.5, 1, 0), levels = c(0, 1))

# âº **è®¡ç®—æ··æ·†çŸ©é˜µ**
conf_matrix_logit <- confusionMatrix(logit_pred, test_data$outcome, positive = "1")
print(conf_matrix_logit)

# â» **è®¡ç®— `ROC Curve` å’Œ `AUC`**
roc_curve_logit <- roc(test_data$outcome, logit_prob)
auc_logit <- auc(roc_curve_logit)
cat("âœ… Logistic Regression AUC:", round(auc_logit, 4), "\n")

# â¼ **ç»˜åˆ¶ `ROC Curve`**
ggplot() +
  geom_line(aes(x = roc_curve_logit$specificities, y = roc_curve_logit$sensitivities), color = "blue") +
  geom_abline(linetype = "dashed") +
  labs(title = "ROC Curve - Logistic Regression", x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()

# â½ **è®­ç»ƒ `Random Forest`**
set.seed(123)
rf_model <- randomForest(outcome ~ ., data = train_data, ntree = 500, mtry = sqrt(ncol(train_data) - 1), importance = TRUE)

# â¾ **é¢„æµ‹ `Test` æ•°æ®**
rf_pred <- predict(rf_model, test_data)

# ğŸ”Ÿ **ç¡®ä¿å› å­ä¸€è‡´**
rf_pred <- factor(rf_pred, levels = levels(test_data$outcome))

# â“« **è®¡ç®—æ··æ·†çŸ©é˜µ**
conf_matrix_rf <- confusionMatrix(rf_pred, test_data$outcome, positive = "1")
print(conf_matrix_rf)

# â“¬ **è®¡ç®— `ROC Curve` å’Œ `AUC`**
rf_prob <- predict(rf_model, test_data, type = "prob")[,2]
roc_curve_rf <- roc(test_data$outcome, rf_prob)
auc_rf <- auc(roc_curve_rf)
cat("âœ… Random Forest AUC:", round(auc_rf, 4), "\n")

# â“­ **ç»˜åˆ¶ `ROC Curve`**
ggplot() +
  geom_line(aes(x = roc_curve_rf$specificities, y = roc_curve_rf$sensitivities), color = "red") +
  geom_abline(linetype = "dashed") +
  labs(title = "ROC Curve - Random Forest", x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()

# â“® **ç»˜åˆ¶ `Feature Importance`**
importance_df <- as.data.frame(importance(rf_model)) %>%
  rownames_to_column(var = "Feature") %>%
  arrange(desc(MeanDecreaseGini))

ggplot(importance_df, aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Feature Importance - Random Forest", x = "Feature", y = "Importance (MeanDecreaseGini)") +
  theme_minimal()
```




XGBoost
```{r}
library(caret)     # æ•°æ®é¢„å¤„ç† & è¯„ä¼°
library(xgboost)   # XGBoost
library(pROC)      # è®¡ç®— AUC & ç”» ROC æ›²çº¿
library(ggplot2)   # å¯è§†åŒ–

# â¶ **ç¡®ä¿ `outcome` æ˜¯ `factor`**
balanced_data$outcome <- as.factor(balanced_data$outcome)

# â· **æ‹†åˆ† `Train/Test` æ•°æ®é›† (80/20)**
set.seed(123)
train_index <- createDataPartition(balanced_data$outcome, p = 0.8, list = FALSE)
train_data <- balanced_data[train_index, ]
test_data <- balanced_data[-train_index, ]

# â¸ **è½¬æ¢æ•°æ®æ ¼å¼ (XGBoost éœ€è¦æ•°å€¼å‹çŸ©é˜µ)**
train_x <- as.matrix(train_data[, -which(names(train_data) == "outcome")])
train_y <- as.numeric(as.character(train_data$outcome))

test_x <- as.matrix(test_data[, -which(names(test_data) == "outcome")])
test_y <- as.numeric(as.character(test_data$outcome))

# â¹ **åˆ›å»º `XGBoost` è®­ç»ƒæ•°æ®**
dtrain <- xgb.DMatrix(data = train_x, label = train_y)
dtest <- xgb.DMatrix(data = test_x, label = test_y)

# âº **è®¾å®š `XGBoost` å‚æ•°**
xgb_params <- list(
  objective = "binary:logistic",  # ç›®æ ‡: äºŒåˆ†ç±»
  eval_metric = "auc",            # è¯„ä¼°æŒ‡æ ‡: AUC
  max_depth = 2,                   # âš ï¸ è®©æ ‘å˜æµ…ï¼ˆé»˜è®¤ 6ï¼‰â¡ï¸ é™ä½å­¦ä¹ èƒ½åŠ›
  eta = 0.5,                        # âš ï¸ è®©å­¦ä¹ ç‡å˜å¤§ï¼Œæ›´æ–°å¹…åº¦è¿‡å¿«ï¼Œå®¹æ˜“è·³è¿‡æœ€ä¼˜è§£
  nthread = 2,                      # çº¿ç¨‹æ•°
  subsample = 0.5,                  # âš ï¸ è®©è®­ç»ƒæ•°æ®é‡‡æ ·æ›´å°‘ï¼Œä¸¢å¤±ä¿¡æ¯ï¼ˆé»˜è®¤ 0.8ï¼‰
  colsample_bytree = 0.5            # âš ï¸ è®©æ¯æ£µæ ‘ç”¨æ›´å°‘ç‰¹å¾ï¼ˆé»˜è®¤ 0.8ï¼‰
)


# â» **è®­ç»ƒ `XGBoost` æ¨¡å‹**
set.seed(123)
xgb_model <- xgb.train(
  params = xgb_params,
  data = dtrain,
  nrounds = 60,      # è®­ç»ƒ 200 è½®
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 10,  # å¦‚æœ AUC 10 è½®ä¸æå‡ï¼Œæå‰åœæ­¢
  verbose = 1
)

# â¼ **é¢„æµ‹ `Test` æ•°æ®**
xgb_prob <- predict(xgb_model, dtest)

# â½ **è½¬æ¢æ¦‚ç‡ä¸ºäºŒåˆ†ç±»é¢„æµ‹**
xgb_pred <- ifelse(xgb_prob > 0.5, 1, 0)
xgb_pred <- factor(xgb_pred, levels = c(0, 1))
test_data$outcome <- factor(test_data$outcome, levels = c(0, 1))

# â¾ **è®¡ç®—æ··æ·†çŸ©é˜µ**
conf_matrix_xgb <- confusionMatrix(xgb_pred, test_data$outcome, positive = "1")
print(conf_matrix_xgb)

# ğŸ”Ÿ **æå–æ¨¡å‹è¯„ä¼°æŒ‡æ ‡**
accuracy_xgb <- conf_matrix_xgb$overall["Accuracy"]
precision_xgb <- conf_matrix_xgb$byClass["Precision"]
recall_xgb <- conf_matrix_xgb$byClass["Sensitivity"]
specificity_xgb <- conf_matrix_xgb$byClass["Specificity"]
f1_score_xgb <- conf_matrix_xgb$byClass["F1"]

cat("\nğŸ“Œ **XGBoost Performance Metrics**:\n")
cat("âœ… Accuracy:", round(accuracy_xgb, 4), "\n")
cat("âœ… Precision:", round(precision_xgb, 4), "\n")
cat("âœ… Recall (Sensitivity):", round(recall_xgb, 4), "\n")
cat("âœ… Specificity:", round(specificity_xgb, 4), "\n")
cat("âœ… F1 Score:", round(f1_score_xgb, 4), "\n")

# ğŸ”Ÿ **è®¡ç®— `ROC Curve` å’Œ `AUC`**
roc_curve_xgb <- roc(test_data$outcome, xgb_prob)
auc_xgb <- auc(roc_curve_xgb)
cat("âœ… AUC:", round(auc_xgb, 4), "\n")

# ğŸ”Ÿ **ç»˜åˆ¶ `ROC Curve`**
ggplot() +
  geom_line(aes(x = roc_curve_xgb$specificities, y = roc_curve_xgb$sensitivities), color = "green") +
  geom_abline(linetype = "dashed") +
  labs(title = "ROC Curve - XGBoost", x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()






# ğŸ“Œ Extract Feature Importance
importance_matrix <- xgb.importance(feature_names = colnames(train_x), model = xgb_model)

# ğŸ“Œ Display Top Features
print(importance_matrix)

# ğŸ“Œ Plot Feature Importance
ggplot(importance_matrix, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity", fill = "darkgreen") +
  coord_flip() +
  labs(title = "Feature Importance - XGBoost",
       x = "Feature",
       y = "Importance (Gain)") +
  theme_minimal()

```





LASSO
```{r}
# ğŸ“Œ åŠ è½½å¿…è¦çš„åº“
library(glmnet)   # LASSO (L1 æ­£åˆ™åŒ–å›å½’)
library(caret)    # æ•°æ®å¤„ç† & è¯„ä¼°
library(pROC)     # AUC è®¡ç®—
library(ggplot2)  # å¯è§†åŒ–

# â¶ **ç¡®ä¿ `outcome` æ˜¯å› å­**
balanced_data$outcome <- as.factor(balanced_data$outcome)

# â· **æ‹†åˆ† `Train/Test` æ•°æ®é›† (80/20)**
set.seed(123)
train_index <- createDataPartition(balanced_data$outcome, p = 0.8, list = FALSE)
train_data <- balanced_data[train_index, ]
test_data <- balanced_data[-train_index, ]

# â¸ **è½¬æ¢æ•°æ®æ ¼å¼ï¼ˆLASSO éœ€è¦æ•°å€¼çŸ©é˜µï¼‰**
train_x <- as.matrix(train_data[, -which(names(train_data) == "outcome")])
train_y <- as.numeric(as.character(train_data$outcome))

test_x <- as.matrix(test_data[, -which(names(test_data) == "outcome")])
test_y <- as.numeric(as.character(test_data$outcome))

# â¹ **ä½¿ç”¨äº¤å‰éªŒè¯å¯»æ‰¾æœ€ä½³ Î»ï¼ˆæ­£åˆ™åŒ–å¼ºåº¦ï¼‰**
set.seed(123)
cv_lasso <- cv.glmnet(train_x, train_y, alpha = 1, family = "binomial", nfolds = 10)
best_lambda <- cv_lasso$lambda.min
cat("âœ… Best Lambda (Regularization Strength):", round(best_lambda, 6), "\n")

# âº **è®­ç»ƒæœ€ç»ˆçš„ LASSO Logistic Regression æ¨¡å‹**
lasso_model <- glmnet(train_x, train_y, alpha = 1, family = "binomial", lambda = best_lambda)

# â» **é¢„æµ‹ `Test` æ•°æ®**
lasso_prob <- predict(lasso_model, newx = test_x, type = "response")

# â¼ **è½¬æ¢æ¦‚ç‡ä¸ºäºŒåˆ†ç±»é¢„æµ‹**
lasso_pred <- ifelse(lasso_prob > 0.5, 1, 0)
lasso_pred <- factor(lasso_pred, levels = c(0, 1))
test_data$outcome <- factor(test_data$outcome, levels = c(0, 1))

# â½ **è®¡ç®—æ··æ·†çŸ©é˜µ**
conf_matrix_lasso <- confusionMatrix(lasso_pred, test_data$outcome, positive = "1")
print(conf_matrix_lasso)

# â¾ **æå–æ¨¡å‹è¯„ä¼°æŒ‡æ ‡**
accuracy_lasso <- conf_matrix_lasso$overall["Accuracy"]
precision_lasso <- conf_matrix_lasso$byClass["Precision"]
recall_lasso <- conf_matrix_lasso$byClass["Sensitivity"]
specificity_lasso <- conf_matrix_lasso$byClass["Specificity"]
f1_score_lasso <- conf_matrix_lasso$byClass["F1"]

cat("\nğŸ“Œ **LASSO Logistic Regression Performance Metrics**:\n")
cat("âœ… Accuracy:", round(accuracy_lasso, 4), "\n")
cat("âœ… Precision:", round(precision_lasso, 4), "\n")
cat("âœ… Recall (Sensitivity):", round(recall_lasso, 4), "\n")
cat("âœ… Specificity:", round(specificity_lasso, 4), "\n")
cat("âœ… F1 Score:", round(f1_score_lasso, 4), "\n")

# ğŸ”Ÿ **è®¡ç®— `ROC Curve` å’Œ `AUC`**
roc_curve_lasso <- roc(test_data$outcome, lasso_prob)
auc_lasso <- auc(roc_curve_lasso)
cat("âœ… AUC:", round(auc_lasso, 4), "\n")

# ğŸ”Ÿ **ç»˜åˆ¶ `ROC Curve`**
ggplot() +
  geom_line(aes(x = roc_curve_lasso$specificities, y = roc_curve_lasso$sensitivities), color = "purple") +
  geom_abline(linetype = "dashed") +
  labs(title = "ROC Curve - LASSO Logistic Regression", x = "1 - Specificity", y = "Sensitivity") +
  theme_minimal()

```












